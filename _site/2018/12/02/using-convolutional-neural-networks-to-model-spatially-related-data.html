<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>CNN Object Detection</title>
  <meta name="description" content="CIO LEVEL SUMMARY:">
  
  <meta name="author" content="Ben Centra">
  <meta name="copyright" content="&copy; Ben Centra 2020">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/monokai-sublime.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  
  <!-- Facebook OGP cards -->
  <meta property="og:description" content="CIO LEVEL SUMMARY:" />
  <meta property="og:url" content="http://localhost:4000/2018/12/02/using-convolutional-neural-networks-to-model-spatially-related-data.html">
  <meta property="og:site_name" content="Raft" />
  <meta property="og:title" content="CNN Object Detection" />
  <meta property="og:type" content="website" />
  <meta property="og:image" content="http://localhost:4000/assets/logo.png" />
  <meta property="og:image:type" content="image/png" />
  <meta property="og:image:width" content="612" />
  <meta property="og:image:height" content="605" />
  

  
  <!-- Twitter: card tags -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="CNN Object Detection">
  <meta name="twitter:description" content="CIO LEVEL SUMMARY:">
  <meta name="twitter:image" content="http://localhost:4000/assets/logo.png">
  <meta name="twitter:url" content="http://localhost:4000/2018/12/02/using-convolutional-neural-networks-to-model-spatially-related-data.html">
  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/2018/12/02/using-convolutional-neural-networks-to-model-spatially-related-data.html">
	<link rel="alternate" type="application/rss+xml" title="Raft" href="http://localhost:4000/feed.xml" />

	<!-- Tooltips -->
	<script type="text/javascript">
		window.tooltips = []
	</script>
</head>


  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <div class="left">
        <img src="/assets/logo.png" alt="Raft">
      </div>
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
				
	
	<li class="nav-link"><a href="/about/">About</a>
	

	
	<li class="nav-link"><a href="/blog/">Blog</a>
	

	
	<li class="nav-link"><a href="/clients/">Clients</a>
	

	
	<li class="nav-link"><a href="/contact/">Contact</a>
	

	

	

	

	
	<li class="nav-link"><a href="/services/">Services</a>
	

	

	

	

	

	


      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">

<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">CNN Object Detection</h1>
      <p class="info">by <strong></strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">December 2, 2018</div>
  <div class="post-categories">
  
  </div>
</section>

<article class="post-content">
  <p><strong>CIO LEVEL SUMMARY:</strong></p>

<ol>
  <li>
    <p>Convolutional Neural Networks (CNN) expand the principles of Feed Forward Neural Networks in order to preserve spatial relationships between input data.</p>
  </li>
  <li>
    <p>CNNs are especially useful for image data, since pixels often have strong relationships with their surrounding pixels.</p>
  </li>
  <li>
    <p>Spatial relationships are preserved by sliding a filter matrix (or kernel) across the matrix and performing the convolution operation. Using the same weights for the filter at each location allows CNNs to recognize features no matter where they are in the image.</p>
  </li>
  <li>
    <p>Convolution filters often help detect features such as edges or curves which can help the CNN classify the image.</p>
  </li>
  <li>
    <p>Pooling is used to reduce the dimensions of the convolved image in order to make it more manageable to use.</p>
  </li>
  <li>
    <p>The output of the convolutional and pooling layers is fed into at least one Feed Forward layer in order to classify the image as desired.</p>
  </li>
</ol>

<p><strong>WHAT ARE CONVOLUTIONAL NEURAL NETS?</strong></p>

<p>Neural Networks are incredibly useful models that can learn complex
relationships between inputs and an output or prediction--like using a
patient’s vital signs to predict risk of heart disease. However, plain
Feed Forward Neural Networks aren’t equipped to handle situations in
which the input data has spatial relationships. Simple Feed Forward
Neural Networks take in one dimensional vectors as input, which
discounts any spatial relationships between these inputs.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture1.png" alt="" class="center-image" /></p>

<p>If you wanted to feed a small black and white image--like a handwritten
digit from the MNIST dataset--into a simple Feed Forward Neural
Network, you could take a matrix of the pixels and flatten it so that
instead of a 28x28 pixel square, you get a single, one dimensional
vector that is 784 pixels long. Below, a 3x3 matrix of pixels is shown
flattened into a one dimensional vector of 9 pixels.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture2.png" alt="" class="center-image" /></p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture3.png" alt="" class="center-image" /></p>

<p>And for smaller, simpler problems like classifying the MNIST digits,
transforming a 2D matrix of pixels into a 1D vector of pixels can be
surprisingly effective.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture4.png" alt="" class="center-image" />
<em>MNIST digits from <a href="https://en.wikipedia.org/wiki/MNIST_database">Wikipedia</a></em></p>

<p>But often, the images that we want to work with are much more
complicated.</p>

<p>When we flatten an image into a 1D vector, we lose information about the
spatial relationships between the pixels. A pixel in one area most
likely has strong relationships with the pixels surrounding it. If we
want to look at an MRI to recognize things like tumors, lesions, or
hippocampal volume, the spatial relationship between pixels is
incredibly important.</p>

<p>Convolutional Neural Networks (CNNs) allow us to take advantage of the
spatial relationships of numbers--whether they’re pixel values or
not--in our neural network architecture. There are three new “steps”
that we can use in a CNN that we didn’t have when we built a Feed
Forward Neural Network in the last article. <strong>Convolution</strong>,
<strong>Detection</strong>, and <strong>Pooling</strong>.</p>

<p><strong>CONVOLUTION</strong></p>

<p>First, we’ll consider a simple black and white fluorescent microscopy
image. This is a mitochondrion from a HeLa cell.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture5.png" alt="" class="center-image" /></p>

<p>We also have images (courtesy of the National Institute on Aging), of 9
other types of organelles.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture6.png" alt="" class="center-image" /></p>

<p>We can use a Convolutional Neural Network in order to classify new
images as one of the 10 types of organelles in this dataset (Nuclei,
Endoplasmic reticulum, cis/medial Golgi, cis Golgi, Lysosomes,
Mitochondria, Nucleoli, Actin, Endosomes, and Microtubules).</p>

<p>We can represent an image as a matrix of pixels. On the left is the
numeric version our our matrix, on the right is the same matrix, but
with the corresponding color for each pixel. 0’s are black, 1’s are
white, and anything in between is a shade of gray.</p>

<table class="nolinetable">
  <tbody>
    <tr>
      <td><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture7.png" alt="" /></td>
      <td><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture8.png" alt="" /></td>
    </tr>
  </tbody>
</table>

<p>To preserve the spatial relationships between the pixels in these
images, we create matrices called <strong>filters</strong> (or kernels) which we can
slide over the matrix of pixels in the image.</p>

<p>A filter that looks for edges might look like this.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture9.png" alt="" class="center-image-md" /></p>

<p>The convolution operation takes the dot product of the filter, and the
matrix of pixels it is on top of. This particular filter is useful,
because the scalar output of the dot product will be LARGE if there is a
big difference between the center pixel and surrounding pixels (i.e. an
edge), and small if there’s a small difference between the center pixel
and those around it (i.e. not an edge).</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture10.png" alt="" class="center-image" /></p>

<p>When we multiply each element of the filter with the value of the pixel
under it, and add all these up, we get a single number, in this case,
-1.41.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture11.png" alt="" class="center-image-md" /></p>

<p>In general, if we call the filter F, and the submatrix M, then
convolution is</p>

<script type="math/tex; mode=display">\sum\limits_{i = 1}^{n} \sum\limits_{j = 1}^{n} F_{ij}*M_{ij}%0</script>

<p>We then slide the filter to the next position, take the dot product, and
repeat. To decide the next position, we have to define the <strong>stride</strong>,
which tells us how far to move the filter for the next calculation. Here
we have a stride of 1 since we’re moving the filter by one pixel each
time.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture12.png" alt="" class="center-image" /></p>

<p>In this case, our matrix of pixels is 10x10, and our filter is 3x3, so
with a stride of 1, our output will be 8x8. In general, the dimensions
with matrix Width W, Stride S and Filter width F will be <script type="math/tex">((W-F)/S)  + 1</script>
But often, we want our output to be the same size as our input. So we
surround our image with a “padding” of 0’s. Our original matrix looks
like this:</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture13.png" alt="" class="center-image-md" /></p>

<p>To pad it, we add a row of 0’s at the top and bottom, and a column of
0’s at either side.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture14.png" alt="" class="center-image-md" /></p>

<p>This allows our output to have the same dimensions as our original
input, which is often useful when working with images.</p>

<p>Using our filter and our padded matrix, we get this output.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture15.png" alt="" class="center-image-md" /></p>

<p>At the (2,2) location, you can see the -1.41 that we calculated above.</p>

<p>Plotting a rescaled version of this matrix (so that each value is
between 0 and 1), we can see that this filter does detect edges
relatively well.</p>

<table class="nolinetable">
  <tbody>
    <tr>
      <td><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture16.png" alt="" class="center-image-md" /></td>
      <td><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture17.png" alt="" class="center-image-md" /></td>
    </tr>
  </tbody>
</table>

<p>Now, we’ve completed the convolution step. But, just like with Feed
Forward Neural Networks, we often want to use some kind of activation
function to scale our output and introduce some non-linearity.</p>

<p>Sigmoid activations like the logistic and tanh activations are often
useful, as well as the ever popular Rectified Linear Unit (ReLu).
Applying these functions elementwise to our output matrix is often
referred to as the <strong>Detector Step</strong> of a Convolutional Layer.</p>

<p>Edge detection filters are usually simple and easy to understand. But in
a CNN, the filters aren’t explicitly chosen, they’re <em>learned</em> by the
network through training.</p>

<p>The values of the filter matrix are often referred to as <em>weights</em>. And
these weights are often randomly initialized and then tweaked through
backpropagation or similar training methods.</p>

<p>The weights of the filter are tied, or shared. This means that they
don’t change as the filter slides over the image, which results in a
phenomenon called <strong>spatial invariance</strong>. This is really useful, since
it allows the CNN to recognize patterns or objects no matter where they
are in the image, because it’s using the same filter everywhere. It also
reduces the amount of weights (or parameters) that need to be optimized.
Less calculations means that the CNN can be trained both faster, and
often with less data.</p>

<p>In our organelle classification example, we want to recognize a
mitochondrion no matter if it’s in the center, or slightly offset.</p>

<p><strong>POOLING</strong></p>

<p>Convolution helps extract useful features from images. But when we’re
working with large complex images, we often want to reduce the amount of
features we have. To do this, we use <strong>Pooling</strong>. Pooling reduces the
dimensions of our output matrices so that we can work with smaller sets
of features.</p>

<p>Two popular forms of pooling are max pooling, and average pooling.</p>

<p>Similarly to the Convolution Step, in the Pooling Step we need to
specify the size of the filter that we want to use, and the stride. For
example, here, we’ll look at 2x2 sections of our matrix, and our stride
will be 2.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture18.png" alt="" class="center-image" /></p>

<p>In max pooling, we take the maximum value of each 2x2 section, and
create a new, smaller matrix.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture19.png" alt="" class="center-image" /></p>

<p>In average pooling, we take the average of each value in the 2x2 section
to create our new matrix.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture20.png" alt="" class="center-image" /></p>

<p>Like with the Convolution step, the output of pooling on a Matrix with
width W, with Filter width F, and Stride S will be
<script type="math/tex">((W-F)/S)  + 1</script> or 2x2 in this case.</p>

<p>In practice, we often use more than one convolution and one pooling
layer in our network. We can layer convolution, detection (activation),
and pooling layers to create Deep Convolutional architectures.</p>

<p>In many cases, we want to use Convolutional layers to generate
“features” that we can then feed into one or more Feed Forward Layers
that produce an output, like a decision about which of 10 organelles is
in an unseen image. So the last few layers of a CNN look similar to the
last few layers of a Feed Forward Neural Network.</p>

<p><img src="//healthstarinfo.com/blogs/assets/images/convolutional_nn/Picture21.png" alt="" class="center-image" /></p>

<p><strong>APPLICATIONS</strong></p>

<p>Let’s build a Convolutional Neural Network in Python to classify our
organelle data.</p>

<p>First, the necessary import statements.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">img_to_array</span><span class="p">,</span> <span class="n">ImageDataGenerator</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Conv2D</span><span class="p">,</span> <span class="n">MaxPooling2D</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Flatten</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">SGD</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">random</span>
</code></pre></div></div>
<p>We need a few packages and functions to build a Convolutional Neural
Network, each one will be explained as needed.</p>

<p>Before we start, we’ll define variables and a few useful functions that
will help us to load our organelle images into Python.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># variable definitions------------------------------
</span><span class="n">nclass</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1">#fun definitions------------------------------------
</span><span class="k">def</span> <span class="nf">load_image</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="c1"># load the image
</span>    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">path</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">))</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">img_to_array</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">image</span>
<span class="k">def</span> <span class="nf">load_images</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="n">d2</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span> <span class="c1">#set seed in order to create reproducibility
</span>    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">d</span><span class="p">:</span> <span class="c1">#for each key in the dictionary
</span>        <span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">d</span><span class="p">[</span><span class="n">k</span><span class="p">])</span> <span class="c1">#randomize image order
</span>        <span class="n">imagesList</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">d</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span> <span class="c1">#list
</span>            <span class="n">imagesList</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">load_image</span><span class="p">(</span><span class="n">item</span><span class="p">))</span>
        <span class="n">d2</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">imagesList</span>
    <span class="k">return</span> <span class="n">d2</span>
</code></pre></div></div>

<p>Next, we will find our images and load them into Python using the
functions we just created.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># get images ---------------------------------------
</span><span class="n">foldNames</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s">"hela/*"</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">folder</span> <span class="ow">in</span> <span class="n">foldNames</span><span class="p">:</span>
    <span class="c1"># get folder names
</span>    <span class="n">labelNames</span> <span class="o">=</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">folder</span><span class="o">+</span><span class="s">"/*.TIF"</span><span class="p">)</span> <span class="o">+</span> <span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="n">folder</span><span class="o">+</span><span class="s">"/*.tif"</span><span class="p">)</span>
    <span class="n">d</span><span class="p">[</span><span class="n">folder</span><span class="p">[</span><span class="mi">5</span><span class="p">:]]</span> <span class="o">=</span> <span class="n">labelNames</span>

<span class="n">ims</span> <span class="o">=</span> <span class="n">load_images</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>

<span class="n">images</span> <span class="o">=</span> <span class="n">ims</span><span class="p">[</span><span class="s">"dna"</span><span class="p">]</span> <span class="o">+</span>\
 <span class="n">ims</span><span class="p">[</span><span class="s">"mitochondria"</span><span class="p">]</span> <span class="o">+</span>\
 <span class="n">ims</span><span class="p">[</span><span class="s">"actin"</span><span class="p">]</span> <span class="o">+</span>\
 <span class="n">ims</span><span class="p">[</span><span class="s">"endosome"</span><span class="p">]</span> <span class="o">+</span>\
 <span class="n">ims</span><span class="p">[</span><span class="s">"er"</span><span class="p">]</span> <span class="o">+</span>\
 <span class="n">ims</span><span class="p">[</span><span class="s">"golgia"</span><span class="p">]</span> <span class="o">+</span>\
 <span class="n">ims</span><span class="p">[</span><span class="s">"golgpp"</span><span class="p">]</span> <span class="o">+</span>\
 <span class="n">ims</span><span class="p">[</span><span class="s">"lysosome"</span><span class="p">]</span> <span class="o">+</span>\
 <span class="n">ims</span><span class="p">[</span><span class="s">"microtubules"</span><span class="p">]</span> <span class="o">+</span>\
 <span class="n">ims</span><span class="p">[</span><span class="s">"nucleolus"</span><span class="p">]</span>

<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ims</span><span class="p">[</span><span class="s">"dna"</span><span class="p">]]</span> <span class="o">+</span>\
 <span class="p">[</span><span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ims</span><span class="p">[</span><span class="s">"mitochondria"</span><span class="p">]]</span> <span class="o">+</span>\
 <span class="p">[</span><span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ims</span><span class="p">[</span><span class="s">"actin"</span><span class="p">]]</span> <span class="o">+</span>\
 <span class="p">[</span><span class="mi">3</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ims</span><span class="p">[</span><span class="s">"endosome"</span><span class="p">]]</span> <span class="o">+</span>\
 <span class="p">[</span><span class="mi">4</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ims</span><span class="p">[</span><span class="s">"er"</span><span class="p">]]</span> <span class="o">+</span>\
 <span class="p">[</span><span class="mi">5</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ims</span><span class="p">[</span><span class="s">"golgia"</span><span class="p">]]</span> <span class="o">+</span>\
 <span class="p">[</span><span class="mi">6</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ims</span><span class="p">[</span><span class="s">"golgpp"</span><span class="p">]]</span> <span class="o">+</span>\
 <span class="p">[</span><span class="mi">7</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ims</span><span class="p">[</span><span class="s">"lysosome"</span><span class="p">]]</span> <span class="o">+</span>\
 <span class="p">[</span><span class="mi">8</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ims</span><span class="p">[</span><span class="s">"microtubules"</span><span class="p">]]</span> <span class="o">+</span>\
 <span class="p">[</span><span class="mi">9</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ims</span><span class="p">[</span><span class="s">"nucleolus"</span><span class="p">]]</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="s">'float'</span><span class="p">)</span> <span class="o">/</span> <span class="mf">255.0</span> <span class="c1">#for color ims
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
</code></pre></div></div>
<p>Now that our images our loaded into the array <em>data</em> and our labels (a
number indicating which organelle the picture contains) in the array
<em>labels</em>, we’re ready to start preparing to build a model.</p>

<p>First, we’ll need to create a training set and a testing set. We’ll use
the training set in order to train our CNN, and the testing set in order
to check how accurately our model performs on data it had never seen
before.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Split data into training and testing sets----------------
</span><span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span> <span class="c1"># use 25% of the data for test
</span>
<span class="n">trainY</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">train_labels</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">nclass</span><span class="p">)</span>
<span class="n">testY</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">test_labels</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">nclass</span><span class="p">)</span>
</code></pre></div></div>

<p>Convolutional Neural Networks often require a large amount of data to
train. While we have hundreds of images, it might be helpful to have
more to work with.</p>

<p>In order to “create” more data to train our CNN, we’ll use the
<em>ImageDataGenerator()</em> function from keras. This function uses our
existing data in order to generate new images for us to train our model
with. One way that the generator can create new images is by rotating or
shifting existing images. This not only provides new data points, but
also helps the network to learn to recognize the organelles no matter
where they are in the image. Check out the documentation for this
function to see a more in depth explanation of how it works.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Augment Small Dataset--------------------------------------
</span><span class="n">aug</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">rotation_range</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">width_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">height_shift_range</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">horizontal_flip</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fill_mode</span><span class="o">=</span><span class="s">"nearest"</span><span class="p">)</span>
<span class="n">aug</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
</code></pre></div></div>

<p>Once we have our training set, we can specify the architecture of our
Convolutional Neural Network. We’ll do this using a function called
<em>createModel()</em>. Inside this function, we’ll specify the architecture of
our CNN.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">createModel</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

    <span class="c1"># CONVOLUTIONAL LAYER 1
</span>    <span class="c1"># 32 filters, size 4x4
</span>    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">depth</span><span class="p">),</span> <span class="n">name</span> <span class="o">=</span><span class="s">"conv_1"</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">"relu"</span><span class="p">))</span>

    <span class="c1"># CONVOLUTIONAL LAYER 1
</span>    <span class="c1"># 64 filters, size 4x4
</span>    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span> <span class="n">name</span> <span class="o">=</span><span class="s">"conv_2"</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">"relu"</span><span class="p">))</span>

    <span class="c1">#POOLING
</span>    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling2D</span><span class="p">(</span><span class="n">pool_size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)))</span>

    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span> <span class="c1">#avoid overfitting
</span>
    <span class="c1"># fully connected layer
</span>    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'relu'</span><span class="p">))</span>
</code></pre></div></div>
<p>First, we create a base model using <em>Sequential()</em>, then we keep adding
layers to our model. Our first Convolutional layer uses 32 filters, each
of size 4x4. Python will pad our images with 0’s so that the output from
the convolution is the same size as the original input.</p>

<p>Then, the output of the convolution is fed into a ReLu activation.</p>

<p>Next we add a second convolutional layer. This time with 64 4x4 filters.
Again, our images will be padded so that the input and output of the
convolution have the same dimensions. We feed this output through a ReLu
activation function, and finally we reach our Pooling Step.</p>

<p>Here, we’ll use Max Pooling, with size 2x2, and stride 1.</p>

<p>Now that we’re done with the convolutional parts of the network, we use
the function <em>Flatten()</em> in order to turn the output of our
convolutional layers into a one dimensional vector. After applying
another ReLu activation, we feed this--now one dimensional--vector
into a normal feed forward layer in order to get the organelle’s
classification. We use a softmax activation at the last layer in order
to get probabilities for each class of organelle.</p>

<p>We’ve laid the groundwork for our model, now we need to ask Python to
train it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m</span> <span class="o">=</span> <span class="n">createModel</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">nclass</span><span class="p">)</span>
<span class="n">b_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">e</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># train the network
</span><span class="k">print</span><span class="p">(</span><span class="s">"Begin Training....</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">aug</span><span class="o">.</span><span class="n">flow</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">trainY</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">b_size</span><span class="p">),</span>
	<span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">testY</span><span class="p">),</span>
	<span class="n">epochs</span><span class="o">=</span><span class="n">e</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>
<p>Since we’re using stochastic gradient descent (which approximates the
gradient of the loss function using only a small sample), we need to
specify a batch size. Here, we chose 32.</p>

<p>We also need to define the number of epochs to use when training our
model. An epoch refers to a full pass of the training data through the
network. That means that each image in our training set will be sent
through the model.</p>

<p>This model might take a while to train, so if you’re in a rush, you can
reduce the number of epochs that your model will use. But don’t reduce
it too far. The network needs to see a lot of data in order to generate
a well performing model.</p>

<p>Once our model is done running, we’ll be able to see how it performed
(in terms of accuracy) on both our training and testing set (in the
model, this is also called the validation set).</p>

<p><strong>CONCLUSION</strong></p>

<p>The Convolutional Neural Network is a powerful tool for performing image
related tasks. CNNs have been used extensively in the medical and health
fields. They have been used to segment tumors<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>, find brain
lesions<sup id="fnref:2"><a href="#fn:2" class="footnote">2</a></sup>, diagnose skin lesions<sup id="fnref:3"><a href="#fn:3" class="footnote">3</a></sup>, and have even been used in
computer vision to recognize the location of surgical tools during
laparoscopic surgery<sup id="fnref:4"><a href="#fn:4" class="footnote">4</a></sup>. And as these fields increase the amount of
image data they collect, the uses for CNNs will grow.</p>

<p><strong>REFRENCES</strong></p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p>Pereira, S., Pinto, A., Alves, V., &amp; Silva, C. A. (2016). Brain
tumor segmentation using convolutional neural networks in MRI
images. <em>IEEE transactions on medical imaging</em>, <em>35</em>(5), 1240-1251. <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>Kamnitsas, K., Chen, L., Ledig, C., Rueckert, D., &amp; Glocker, B.
(2015). Multi-scale 3D convolutional neural networks for lesion
segmentation in brain MRI. <em>Ischemic stroke lesion segmentation</em>,
<em>13</em>, 46. <a href="#fnref:2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>Kawahara, J., BenTaieb, A., &amp; Hamarneh, G. (2016, April). Deep
features to classify skin lesions. In <em>Biomedical Imaging (ISBI),
2016 IEEE 13th International Symposium on</em> (pp. 1397-1400). IEEE. <a href="#fnref:3" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:4">
      <p>Choi, B., Jo, K., Choi, S., &amp; Choi, J. (2017, July).
Surgical-tools detection based on Convolutional Neural Network in
laparoscopic robot-assisted surgery. In <em>Engineering in Medicine and
Biology Society (EMBC), 2017 39th Annual International Conference of
the IEEE</em> (pp. 1756-1759). IEEE. <a href="#fnref:4" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

</article>





<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<section class="share">
  <span>Share: </span>
  
    
    
      <a href="//twitter.com/share?text=CNN+Object+Detection&url=http%3A%2F%2Flocalhost%3A4000%2F2018%2F12%2F02%2Fusing-convolutional-neural-networks-to-model-spatially-related-data.html&via=TheBenCentra"
        onclick="window.open(this.href, 'twitter-share', 'width=550,height=255');return false;">
        <i class="fa fa-twitter-square fa-lg"></i>
      </a>
    
    
    
    
    
    
    
  
    
    
    
      <a href="//www.facebook.com/sharer.php?t=CNN+Object+Detection&u=http%3A%2F%2Flocalhost%3A4000%2F2018%2F12%2F02%2Fusing-convolutional-neural-networks-to-model-spatially-related-data.html"
        onclick="window.open(this.href, 'facebook-share', 'width=550,height=255');return false;">
        <i class="fa fa-facebook-square fa-lg"></i>
      </a>
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
      <a href="//www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2F2018%2F12%2F02%2Fusing-convolutional-neural-networks-to-model-spatially-related-data.html"
        onclick="window.open(this.href, 'linkedin-share', 'width=550,height=255');return false;">
        <i class="fa fa-linkedin-square fa-lg"></i>
      </a>
    
    
    
    
  
    
    
    
    
      <a href="//plus.google.com/share?title=CNN+Object+Detection&url=http%3A%2F%2Flocalhost%3A4000%2F2018%2F12%2F02%2Fusing-convolutional-neural-networks-to-model-spatially-related-data.html"
        onclick="window.open(this.href, 'google-plus-share', 'width=550,height=255');return false;">
        <i class="fa fa-google-plus-square fa-lg"></i>
      </a>
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
      <a href="//www.pinterest.com/pin/create/button/?description=CNN+Object+Detection&url=http%3A%2F%2Flocalhost%3A4000%2F2018%2F12%2F02%2Fusing-convolutional-neural-networks-to-model-spatially-related-data.html&media=http://localhost:4000/assets/header_image.jpg"
        onclick="window.open(this.href, 'pinterest-share', 'width=550,height=255');return false;">
        <i class="fa fa-pinterest-square fa-lg"></i>
      </a>
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
    
    
      <a href="//www.reddit.com/submit" onclick="window.location = '//www.reddit.com/submit?url=' + encodeURIComponent('http://localhost:4000/2018/12/02/using-convolutional-neural-networks-to-model-spatially-related-data.html') + '&title=CNN Object Detection'; return false">
        <i class="fa fa-reddit-square fa-lg"></i>
      </a>
    
    
  
    
    
    
    
    
    
    
    
  
</section>




</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <div class="footer-logo site-contact">
      <img src="/assets/raft-name-logo.png" />
    </div>

    <div class="site-contact">

      <div class="address-footer">
        1801 Reston Pkwy <br />
        Suite #102 <br />
        Reston, VA 20190
      </div>

        
          
            <a href="" title="Follow me on Twitter">
              <i class="fa fa-twitter"></i>
            </a>
          
        
          
            <a href="http://" title="Friend me on Facebook">
              <i class="fa fa-facebook"></i>
            </a>
          
        
          
            <a href="https://github.com/bencentra" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
            </a>
          
        
          
            <a href="https://www.linkedin.com/pub/ben-centra/47/769/60a" title="Connect with me on LinkedIn">
              <i class="fa fa-linkedin"></i>
            </a>
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        

    </div>

    <div class="site-contact">

      <div class="address-footer-double">
        <a href="tel:5715264106"><i class="fa fa-phone"></i>(571)-526-4106</a><br />
        <a href="tel:5715264106"><i class="fa fa-fax"></i>(571)-526-4106</a><br />
        <a href="mailto:info@goraft.tech?Subject=goraft.tech%20contact"><i class="fa fa-envelope"></i>info@goraft.tech</a>
      </div>

    </div>

  </div>

</footer>

<div id="copyright">&copy; Raft 2020</div>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-3.4.1.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.1/js/lightbox.min.js"></script>
<script src="//unpkg.com/popper.js@1"></script>
<script src="//unpkg.com/tippy.js@5"></script>

<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();

  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });

	// Enable tooltips via Tippy.js
	if (Array.isArray(window.tooltips)) {
		window.tooltips.forEach(function(tooltip) {
			var selector = tooltip[0];
			var config = tooltip[1];
			tippy(selector, config);
		})
	}
});

</script>






  </body>

</html>
